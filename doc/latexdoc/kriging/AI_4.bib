@article{Baker1986,
author = {Baker, Jon},
doi = {10.1002/jcc.540070402},
file = {:home/gerardo/Dropbox/Sweden/papers/An algorithm for location of transition states, Backer.pdf:pdf},
issn = {0192-8651},
journal = {Journal of Computational Chemistry},
month = {aug},
number = {4},
pages = {385--395},
title = {{An algorithm for the location of transition states}},
url = {http://doi.wiley.com/10.1002/jcc.540070402},
volume = {7},
year = {1986}
}
@book{Laurent2019,
abstract = {{\textcopyright} 2017 CIMNE, Barcelona, Spain Metamodeling, the science of modeling functions observed at a finite number of points, benefits from all auxiliary information it can account for. Function gradients are a common auxiliary information and are useful for predicting functions with locally changing behaviors. This article is a review of the main metamodels that use function gradients in addition to function values. The goal of the article is to give the reader both an overview of the principles involved in gradient-enhanced metamodels while also providing insightful formulations. The following metamodels have gradient-enhanced versions in the literature and are reviewed here: classical, weighted and moving least squares, Shepard weighting functions, and the kernel-based methods that are radial basis functions, kriging and support vector machines. The methods are set in a common framework of linear combinations between a priori chosen functions and coefficients that depend on the observations. The characteristics common to all kernel-based approaches are underlined. A new (Formula presented.)-GSVR metamodel which uses gradients is given. Numerical comparisons of the metamodels are carried out for approximating analytical test functions. The experiments are replicable, as they are performed with an opensource available toolbox. The results indicate that there is a trade-off between the better computing time of least squares methods and the larger versatility of kernel-based approaches.},
author = {Laurent, Luc and {Le Riche}, Rodolphe and Soulier, Bruno and Boucard, Pierre Alain},
booktitle = {Archives of Computational Methods in Engineering},
doi = {10.1007/s11831-017-9226-3},
file = {:home/gerardo/Dropbox/Sweden/papers/An overview of gradient-enhanced metamodels with applications.pdf:pdf},
isbn = {0123456789},
issn = {18861784},
number = {1},
pages = {61--106},
publisher = {Springer Netherlands},
title = {{An Overview of Gradient-Enhanced Metamodels with Applications}},
url = {http://dx.doi.org/10.1007/s11831-017-9226-3},
volume = {26},
year = {2019}
}
@article{Elton2018,
abstract = {We present a proof of concept that machine learning techniques can be used to predict the properties of CNOHF energetic molecules from their molecular structures. We focus on a small but diverse dataset consisting of 109 molecular structures spread across ten compound classes. Up until now, candidate molecules for energetic materials have been screened using predictions from expensive quantum simulations and thermochemical codes. We present a comprehensive comparison of machine learning models and several molecular featurization methods - sum over bonds, custom descriptors, Coulomb matrices, bag of bonds, and fingerprints. The best featurization was sum over bonds (bond counting), and the best model was kernel ridge regression. Despite having a small data set, we obtain acceptable errors and Pearson correlations for the prediction of detonation pressure, detonation velocity, explosive energy, heat of formation, density, and other properties out of sample. By including another dataset with 309 additional molecules in our training we show how the error can be pushed lower, although the convergence with number of molecules is slow. Our work paves the way for future applications of machine learning in this domain, including automated lead generation and interpreting machine learning models to obtain novel chemical insights.},
author = {Elton, Daniel C. and Boukouvalas, Zois and Butrico, Mark S. and Fuge, Mark D. and Chung, Peter W.},
doi = {10.1038/s41598-018-27344-x},
file = {:home/gerardo/Dropbox/Sweden/papers/Applying machine learning techniques to predict the properties of energetic materials.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--12},
title = {{Applying machine learning techniques to predict the properties of energetic materials}},
volume = {8},
year = {2018}
}
@article{Dalbey2013,
abstract = {“Naive” or straight-forward Kriging implementations can often perform poorly in practice. The relevant features of the robustly accurate and efficient Kriging and Gradient Enhanced Kriging (GEK) implementations in the DAKOTA software package are detailed herein. The principal contribution is a novel, effective, and efficient approach to handle ill-conditioning of GEK's “correlation” matrix, R{\~{N}}, based on a pivoted Cholesky factorization of Kriging's (not GEK's) correlation matrix, R, which is a small sub-matrix within GEK's R{\~{N}} matrix. The approach discards sample points/equations that contribute the least “new” information to R{\~{N}}. Since these points contain the least new information, they are the ones which when discarded are both the easiest to predict and provide maximum improvement of R{\~{N}}'s conditioning. Prior to this work, handling ill-conditioned correlation matrices was a major, perhaps the principal, unsolved challenge necessary for robust and efficient GEK emulators. Numerical results demonstrate that GEK predictions can be significantly more accurate when GEK is allowed to discard points by the presented method. Numerical results also indicate that GEK can be used to break the curse of dimensionality by exploiting inexpensive derivatives (such as those provided by automatic differentiation or adjoint techniques), smoothness in the response being modeled, and adaptive sampling. Development of a suitable adaptive sampling algorithm was beyond the scope of this work; instead adaptive sampling was approximated by omitting the cost of samples discarded by the presented pivoted Cholesky approach.},
author = {Dalbey, Keith R},
file = {:home/gerardo/Dropbox/Sweden/papers/Efficient and Robust Gradient Enhanced Kriging Emulators.pdf:pdf},
number = {August},
title = {{Efficient and Robust Gradient Enhanced Kriging Emulators}},
url = {https://www.google.com/url?sa=t{\&}rct=j{\&}q={\&}esrc=s{\&}source=web{\&}cd=4{\&}cad=rja{\&}uact=8{\&}ved=0ahUKEwjT3ribqsLSAhWpzIMKHWQjB-IQFggtMAM{\&}url=http{\%}3A{\%}2F{\%}2Fprod.sandia.gov{\%}2Ftechlib{\%}2Faccess-control.cgi{\%}2F2013{\%}2F137022.pdf{\&}usg=AFQjCNGfWMoRRM6VCtBbUkG-1kz7NmFFUg{\&}sig2=cZ-},
year = {2013}
}
@article{Zheng2017,
abstract = {An efficient geometry optimization algorithm based on interpolated potential energy surfaces with iteratively updated Hessians is presented in this work. At each step of geometry optimization (including both minimization and transition structure search), an interpolated potential energy surface is properly constructed by using the previously calculated information (energies, gradients, and Hessians/updated Hessians), and Hessians of the two latest geometries are updated in an iterative manner. The optimized minimum or transition structure on the interpolated surface is used for the starting geometry of the next geometry optimization step. The cost of searching the minimum or transition structure on the interpolated surface and iteratively updating Hessians is usually negligible compared with most electronic structure single gradient calculations. These interpolated potential energy surfaces are often better representations of the true potential energy surface in a broader range than a local quadratic appr...},
author = {Zheng, Jingjing and Frisch, Michael J.},
doi = {10.1021/acs.jctc.7b00719},
file = {:home/gerardo/Dropbox/Sweden/papers/Efficient geometry minimization and transition structure optimization.pdf:pdf},
issn = {15499626},
journal = {Journal of Chemical Theory and Computation},
number = {12},
pages = {6424--6432},
title = {{Efficient Geometry Minimization and Transition Structure Optimization Using Interpolated Potential Energy Surfaces and Iteratively Updated Hessians}},
volume = {13},
year = {2017}
}
@article{Denzel2018a,
abstract = {We implemented a geometry optimizer based on Gaussian process regression (GPR) to find minimum structures on potential energy surfaces. We tested both a two times differentiable form of the Mat{\'{e}}rn kernel and the squared exponential kernel. The Mat{\'{e}}rn kernel performs much better. We give a detailed description of the optimization procedures. These include overshooting the step resulting from GPR in order to obtain a higher degree of interpolation vs. extrapolation. In a benchmark against the Limited-memory Broyden–Fletcher–Goldfarb–Shanno optimizer of the DL-FIND library on 26 test systems, we found the new optimizer to generally reduce the number of required optimization steps.},
author = {Denzel, Alexander and K{\"{a}}stner, Johannes},
doi = {10.1063/1.5017103},
file = {:home/gerardo/Dropbox/Sweden/papers/Gaussian process regression for geometry optimization.pdf:pdf},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {9},
title = {{Gaussian process regression for geometry optimization}},
volume = {148},
year = {2018}
}
@article{Denzel2018,
abstract = {We implemented a gradient-based algorithm for transition state search which uses Gaussian process regression. Besides a description of the algorithm, we provide a method to find the starting point for the optimization if only the reactant and product minima are known. We perform benchmarks on 27 test systems against the dimer method and partitioned rational function optimization as implemented in the DL-FIND library. We found the new optimizer to significantly decrease the number of required energy and gradient evaluations.},
author = {Denzel, Alexander and K{\"{a}}stner, Johannes},
doi = {10.1021/acs.jctc.8b00708},
file = {:home/gerardo/Dropbox/Sweden/papers/Gaussian Process Regression for Transition State Search.pdf:pdf},
issn = {15499626},
journal = {Journal of Chemical Theory and Computation},
number = {11},
pages = {5777--5786},
title = {{Gaussian Process Regression for Transition State Search}},
volume = {14},
year = {2018}
}
@book{Seeger2004,
abstract = {Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
author = {Seeger, Matthias},
booktitle = {International journal of neural systems},
doi = {10.1142/S0129065704001899},
file = {:home/gerardo/Dropbox/Sweden/papers/Gaussian Processes for Machine Learning:},
isbn = {026218253X},
issn = {0129-0657},
number = {2},
pages = {69--106},
pmid = {15112367},
title = {{Gaussian processes for machine learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15112367},
volume = {14},
year = {2004}
}
@article{Zielinski2017,
abstract = {{\textcopyright} 2017 The Author(s). The geometry optimization of a water molecule with a novel type of energy function called FFLUX is presented, which bypasses the traditional bonded potentials. Instead, topologically-partitioned atomic energies are trained by the machine learning method kriging to predict their IQA atomic energies for a previously unseen molecular geometry. Proof-of-concept that FFLUX's architecture is suitable for geometry optimization is rigorously demonstrated. It is found that accurate kriging models can optimize 2000 distorted geometries to within 0.28 kJ mol -1 of the corresponding ab initio energy, and 50{\%} of those to within 0.05 kJ mol -1 . Kriging models are robust enough to optimize the molecular geometry to sub-noise accuracy, when two thirds of the geometric inputs are outside the training range of that model. Finally, the individual components of the potential energy are analyzed, and chemical intuition is reflected in the independent behavior of the three energy terms E A E intra A (intra-atomic), V AA V cl AA ' (electrostatic) and V AA V x AA ' (exchange), in contrast to standard force fields.},
author = {Zielinski, Fran{\c{c}}ois and Maxwell, Peter I. and Fletcher, Timothy L. and Davie, Stuart J. and {Di Pasquale}, Nicodemo and Cardamone, Salvatore and Mills, Matthew J.L. and Popelier, Paul L.A.},
doi = {10.1038/s41598-017-12600-3},
file = {:home/gerardo/Dropbox/Sweden/papers/Geometry Optimization with Machine Trained Topological Atoms.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--18},
title = {{Geometry Optimization with Machine Trained Topological Atoms}},
volume = {7},
year = {2017}
}
@article{DeBaar2013,
abstract = {{\textcopyright} 2014 by Begell House, Inc. Cokriging is a flexible tool for constructing surrogate models on the outputs of computer models. It can readily incorporate gradient information, in which form it is named gradient-enhanced Kriging (GEK), and promises accurate surrogate models in  {\textgreater} 10 dimensions with a moderate number of sample locations for sufficiently smooth responses. However, GEK suffers from several problems: poor robustness and ill-conditionedness of the surface. Furthermore it is unclear how to account for errors in gradients, which are typically larger than errors in values. In this work we derive GEK using Bayes' Theorem, which gives an useful interpretation of the method, allowing construction of a gradienterror contribution. The Bayesian interpretation suggests the “observation error” as a proxy for errors in the output of the computer model. From this point we derive analytic estimates of robustness of the method, which can easily be used to compute upper bounds on the correlation range and lower bounds on the observation error. We thus see that by including the observation error, treatment of errors and robustness go hand in hand. The resulting GEK method is applied to uncertainty quantification for two test problems.},
author = {de Baar, Jouke H.S. and Dwight, Richard P. and Bijl, Hester},
doi = {10.1615/int.j.uncertaintyquantification.2013006809},
file = {:home/gerardo/Dropbox/Sweden/papers/Improvements to gradient-enhanced kriging using a Bayesian Interpretation.pdf:pdf},
issn = {2152-5080},
journal = {International Journal for Uncertainty Quantification},
keywords = {fluid mechanics,gaussian random fields,maximum likelihood},
number = {3},
pages = {205--223},
title = {{Improvements To Gradient-Enhanced Kriging Using a Bayesian Interpretation}},
volume = {4},
year = {2013}
}
@article{Ulaganathan2016,
abstract = {{\textcopyright} 2015, Springer-Verlag London. The use of surrogate models for approximating computationally expensive simulations has been on the rise for the last two decades. Kriging-based surrogate models are popular for approximating deterministic computer models. In this work, the performance of Kriging is investigated when gradient information is introduced for the approximation of computationally expensive black-box simulations. This approach, known as gradient-enhanced Kriging, is applied to various benchmark functions of varying dimensionality (2D-20D). As expected, results from the benchmark problems show that additional gradient information can significantly enhance the accuracy of Kriging. Gradient-enhanced Kriging provides a better approximation even when gradient information is only partially available. Further comparison between gradient-enhanced Kriging and an alternative formulation of gradient-enhanced Kriging, called indirect gradient-enhanced Kriging, highlights various advantages of directly employing gradient information, such as improved surrogate model accuracy, better conditioning of the correlation matrix, etc. Finally, gradient-enhanced Kriging is used to model 6- and 10-variable fluid–structure interaction problems from bio-mechanics to identify the arterial wall's stiffness.},
author = {Ulaganathan, Selvakumar and Couckuyt, Ivo and Dhaene, Tom and Degroote, Joris and Laermans, Eric},
doi = {10.1007/s00366-015-0397-y},
file = {:home/gerardo/Dropbox/Sweden/papers/Performance study of Gradient Enhanced Kriging.pdf:pdf},
issn = {14355663},
journal = {Engineering with Computers},
keywords = {Fluid structure interaction,Gradient enhancement,Kriging,Surrogate modelling},
number = {1},
pages = {15--34},
publisher = {Springer London},
title = {{Performance study of gradient-enhanced Kriging}},
url = {http://dx.doi.org/10.1007/s00366-015-0397-y},
volume = {32},
year = {2016}
}
