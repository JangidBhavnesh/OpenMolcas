@article{Aquilante2016,
abstract = {In this report, we summarize and describe the recent unique updates and additions to the Molcas quantum chemistry program suite as contained in release version 8. These updates include natural and spin orbitals for studies of magnetic properties, local and linear scaling methods for the Douglas-Kroll-Hess transformation, the generalized active space concept in MCSCF methods, a combination of multiconfigurational wave functions with density functional theory in the MC-PDFT method, additional methods for computation of magnetic properties, methods for diabatization, analytical gradients of state average complete active space SCF in association with density fitting, methods for constrained fragment optimization, large-scale parallel multireference configuration interaction including analytic gradients via the interface to the Columbus package, and approximations of the CASPT2 method to be used for computations of large systems. In addition, the report includes the description of a computational machinery for nonlinear optical spectroscopy through an interface to the QM/MM package Cobramm. Further, a module to run molecular dynamics simulations is added, two surface hopping algorithms are included to enable nonadiabatic calculations, and the DQ method for diabatization is added. Finally, we report on the subject of improvements with respects to alternative file options and parallelization. {\textcopyright} 2015 Wiley Periodicals, Inc.},
author = {Aquilante, Francesco and Autschbach, Jochen and Carlson, Rebecca K. and Chibotaru, Liviu F. and Delcey, Micka{\"{e}}l G. and {De Vico}, Luca and {Fdez. Galv{\'{a}}n}, Ignacio and Ferr{\'{e}}, Nicolas and Frutos, Luis Manuel and Gagliardi, Laura and Garavelli, Marco and Giussani, Angelo and Hoyer, Chad E. and {Li Manni}, Giovanni and Lischka, Hans and Ma, Dongxia and Malmqvist, Per {\AA}ke and M{\"{u}}ller, Thomas and Nenov, Artur and Olivucci, Massimo and Pedersen, Thomas Bondo and Peng, Daoling and Plasser, Felix and Pritchard, Ben and Reiher, Markus and Rivalta, Ivan and Schapiro, Igor and Segarra-Mart{\'{i}}, Javier and Stenrup, Michael and Truhlar, Donald G. and Ungur, Liviu and Valentini, Alessio and Vancoillie, Steven and Veryazov, Valera and Vysotskiy, Victor P. and Weingart, Oliver and Zapata, Felipe and Lindh, Roland},
doi = {10.1002/jcc.24221},
file = {:home/gerardo/Dropbox/Sweden/papers/Molcas.pdf:pdf},
issn = {1096987X},
journal = {Journal of Computational Chemistry},
keywords = {electron correlation,gradients,molecular dynamics,parallelization,relativistic},
number = {5},
pages = {506--541},
title = {{Molcas 8: New capabilities for multiconfigurational quantum chemical calculations across the periodic table}},
volume = {37},
year = {2016}
}
@article{Zheng2017a,
abstract = {An efficient geometry optimization algorithm based on interpolated potential energy surfaces with iteratively updated Hessians is presented in this work. At each step of geometry optimization (including both minimization and transition structure search), an interpolated potential energy surface is properly constructed by using the previously calculated information (energies, gradients, and Hessians/updated Hessians), and Hessians of the two latest geometries are updated in an iterative manner. The optimized minimum or transition structure on the interpolated surface is used for the starting geometry of the next geometry optimization step. The cost of searching the minimum or transition structure on the interpolated surface and iteratively updating Hessians is usually negligible compared with most electronic structure single gradient calculations. These interpolated potential energy surfaces are often better representations of the true potential energy surface in a broader range than a local quadratic appr...},
author = {Zheng, Jingjing and Frisch, Michael J.},
doi = {10.1021/acs.jctc.7b00719},
file = {:home/gerardo/Dropbox/Sweden/papers/Efficient Geometry Minimization and Transition StructureOptimization Using Interpolated Potential Energy Surfaces andIteratively Updated Hessians.pdf:pdf},
issn = {15499626},
journal = {Journal of Chemical Theory and Computation},
number = {12},
pages = {6424--6432},
title = {{Efficient Geometry Minimization and Transition Structure Optimization Using Interpolated Potential Energy Surfaces and Iteratively Updated Hessians}},
volume = {13},
year = {2017}
}
@article{Shanno1970,
author = {Shanno, D . F .},
file = {:home/gerardo/Dropbox/Sweden/papers/Conditioning of Quasi-Newton Methods for Function MinimizationAuthor(s), D. F. Shanno.pdf:pdf},
journal = {Mathematics of Computation},
number = {111},
pages = {647--656},
title = {{Conditioning of Quasi-Newton Methods for Function Minimization}},
url = {https://www.jstor.org/stable/2004840},
volume = {24},
year = {1970}
}
@article{Thøgersen2004,
abstract = {The trust-region self-consistent field (TRSCF) method for optimization of total energy of Hartree-Fock theory and Kohn-Sham (KS) theory. The Fock/Kohn-Sham matrix diagonalization step to obtain density matrix and to determine optimal density matrix in subspace of density matrices were improved in TRSCF method. The local models had same gradient such as total energy at the point of expansion. A monotonic and significant reduction of total energy is ensured in each iteration of TRSCF method by restricting steps of TRSCF model to be inside the trust region. The examples are also discussed where TRSCF method converges monotonically and smoothly.},
author = {Th{\o}gersen, Lea and Olsen, Jeppe and Yeager, Danny and J{\o}rgensen, Poul and Salek, Pawel and Helgaker, Trygve},
doi = {10.1063/1.1755673},
file = {:home/gerardo/Dropbox/Sweden/papers/The trust-region self-consistent field method$\backslash$:Towards a black-box optimization in Hartree–Fock and Kohn–Sham theories.pdf:pdf},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {1},
pages = {16--27},
title = {{The trust-region self-consistent field method: Towards a black-box optimization in Hartree-Fock and Kohn-Sham theories}},
volume = {121},
year = {2004}
}
@article{Schlegel2011,
abstract = {Geometry optimization is an important part of most quantum chemical calculations. This article surveys methods for optimizing equilibrium geometries, locating transition structures, and following reaction paths. The emphasis is on optimizations using quasi-Newton methods that rely on energy gradients, and the discussion includes Hessian updating, line searches, trust radius, and rational function optimization techniques. Single-ended and double-ended methods are discussed for transition state searches. Single-ended techniques include quasi-Newton, reduced gradient following and eigenvector following methods. Double-ended methods include nudged elastic band, string, and growing string methods. The discussions conclude with methods for validating transition states and following steepest descent reaction paths. {\textcopyright} 2011 John Wiley {\&} Sons, Ltd. WIREs Comput Mol Sci 2011 1 790–809 DOI: 10.1002/wcms.34},
author = {Schlegel, H. Bernhard},
doi = {10.1002/wcms.34},
file = {:home/gerardo/Dropbox/Sweden/papers/Schlegel-2011-Wiley{\_}Interdisciplinary{\_}Reviews{\_}{\_}Computational{\_}Molecular{\_}Science.pdf:pdf},
issn = {17590876},
journal = {Wiley Interdisciplinary Reviews: Computational Molecular Science},
number = {5},
pages = {790--809},
title = {{Geometry optimization}},
volume = {1},
year = {2011}
}
@article{Bouhlel2019,
abstract = {Surrogate models provide a low computational cost alternative to evaluating expensive functions. The construction of accurate surrogate models with large numbers of independent variables is currently prohibitive because it requires a large number of function evaluations. Gradient-enhanced kriging has the potential to reduce the number of function evaluations for the desired accuracy when efficient gradient computation, such as an adjoint method, is available. However, current gradient-enhanced kriging methods do not scale well with the number of sampling points due to the rapid growth in the size of the correlation matrix where new information is added for each sampling point in each direction of the design space. They do not scale well with the number of independent variables either due to the increase in the number of hyperparameters that needs to be estimated. To address this issue, we develop a new gradient-enhanced surrogate model approach that drastically reduced the number of hyperparameters through the use of the partial-least squares method that maintains accuracy. In addition, this method is able to control the size of the correlation matrix by adding only relevant points defined through the information provided by the partial-least squares method. To validate our method, we compare the global accuracy of the proposed method with conventional kriging surrogate models on two analytic functions with up to 100 dimensions, as well as engineering problems of varied complexity with up to 15 dimensions. We show that the proposed method requires fewer sampling points than conventional methods to obtain the desired accuracy, or provides more accuracy for a fixed budget of sampling points. In some cases, we get over 3 times more accurate models than a bench of surrogate models from the literature, and also over 3200 times faster than standard gradient-enhanced kriging models.},
archivePrefix = {arXiv},
arxivId = {arXiv:1708.02663v1},
author = {Bouhlel, Mohamed A. and Martins, Joaquim R.R.A.},
doi = {10.1007/s00366-018-0590-x},
eprint = {arXiv:1708.02663v1},
file = {:home/gerardo/Dropbox/Sweden/papers/Gradient-enhanced kriging for high-dimensional problems, Mohamed A. Bouhlel.pdf:pdf},
issn = {14355663},
journal = {Engineering with Computers},
keywords = {Gradient-enhanced kriging,High-dimensions,Partial least squares},
number = {1},
pages = {157--173},
title = {{Gradient-enhanced kriging for high-dimensional problems}},
volume = {35},
year = {2019}
}
@article{Baker1986,
author = {Baker, Jon},
doi = {10.1002/jcc.540070402},
file = {:home/gerardo/Dropbox/Sweden/papers/An algorithm for location of transition states, Backer.pdf:pdf},
issn = {0192-8651},
journal = {Journal of Computational Chemistry},
month = {aug},
number = {4},
pages = {385--395},
title = {{An algorithm for the location of transition states}},
url = {http://doi.wiley.com/10.1002/jcc.540070402},
volume = {7},
year = {1986}
}
@book{Laurent2019,
abstract = {{\textcopyright} 2017 CIMNE, Barcelona, Spain Metamodeling, the science of modeling functions observed at a finite number of points, benefits from all auxiliary information it can account for. Function gradients are a common auxiliary information and are useful for predicting functions with locally changing behaviors. This article is a review of the main metamodels that use function gradients in addition to function values. The goal of the article is to give the reader both an overview of the principles involved in gradient-enhanced metamodels while also providing insightful formulations. The following metamodels have gradient-enhanced versions in the literature and are reviewed here: classical, weighted and moving least squares, Shepard weighting functions, and the kernel-based methods that are radial basis functions, kriging and support vector machines. The methods are set in a common framework of linear combinations between a priori chosen functions and coefficients that depend on the observations. The characteristics common to all kernel-based approaches are underlined. A new (Formula presented.)-GSVR metamodel which uses gradients is given. Numerical comparisons of the metamodels are carried out for approximating analytical test functions. The experiments are replicable, as they are performed with an opensource available toolbox. The results indicate that there is a trade-off between the better computing time of least squares methods and the larger versatility of kernel-based approaches.},
author = {Laurent, Luc and {Le Riche}, Rodolphe and Soulier, Bruno and Boucard, Pierre Alain},
booktitle = {Archives of Computational Methods in Engineering},
doi = {10.1007/s11831-017-9226-3},
file = {:home/gerardo/Dropbox/Sweden/papers/An overview of gradient-enhanced metamodels with applications.pdf:pdf},
isbn = {0123456789},
issn = {18861784},
number = {1},
pages = {61--106},
publisher = {Springer Netherlands},
title = {{An Overview of Gradient-Enhanced Metamodels with Applications}},
url = {http://dx.doi.org/10.1007/s11831-017-9226-3},
volume = {26},
year = {2019}
}
@article{Elton2018,
abstract = {We present a proof of concept that machine learning techniques can be used to predict the properties of CNOHF energetic molecules from their molecular structures. We focus on a small but diverse dataset consisting of 109 molecular structures spread across ten compound classes. Up until now, candidate molecules for energetic materials have been screened using predictions from expensive quantum simulations and thermochemical codes. We present a comprehensive comparison of machine learning models and several molecular featurization methods - sum over bonds, custom descriptors, Coulomb matrices, bag of bonds, and fingerprints. The best featurization was sum over bonds (bond counting), and the best model was kernel ridge regression. Despite having a small data set, we obtain acceptable errors and Pearson correlations for the prediction of detonation pressure, detonation velocity, explosive energy, heat of formation, density, and other properties out of sample. By including another dataset with 309 additional molecules in our training we show how the error can be pushed lower, although the convergence with number of molecules is slow. Our work paves the way for future applications of machine learning in this domain, including automated lead generation and interpreting machine learning models to obtain novel chemical insights.},
author = {Elton, Daniel C. and Boukouvalas, Zois and Butrico, Mark S. and Fuge, Mark D. and Chung, Peter W.},
doi = {10.1038/s41598-018-27344-x},
file = {:home/gerardo/Dropbox/Sweden/papers/Applying machine learning techniques to predict the properties of energetic materials.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--12},
title = {{Applying machine learning techniques to predict the properties of energetic materials}},
volume = {8},
year = {2018}
}
@article{Dalbey2013,
abstract = {“Naive” or straight-forward Kriging implementations can often perform poorly in practice. The relevant features of the robustly accurate and efficient Kriging and Gradient Enhanced Kriging (GEK) implementations in the DAKOTA software package are detailed herein. The principal contribution is a novel, effective, and efficient approach to handle ill-conditioning of GEK's “correlation” matrix, R{\~{N}}, based on a pivoted Cholesky factorization of Kriging's (not GEK's) correlation matrix, R, which is a small sub-matrix within GEK's R{\~{N}} matrix. The approach discards sample points/equations that contribute the least “new” information to R{\~{N}}. Since these points contain the least new information, they are the ones which when discarded are both the easiest to predict and provide maximum improvement of R{\~{N}}'s conditioning. Prior to this work, handling ill-conditioned correlation matrices was a major, perhaps the principal, unsolved challenge necessary for robust and efficient GEK emulators. Numerical results demonstrate that GEK predictions can be significantly more accurate when GEK is allowed to discard points by the presented method. Numerical results also indicate that GEK can be used to break the curse of dimensionality by exploiting inexpensive derivatives (such as those provided by automatic differentiation or adjoint techniques), smoothness in the response being modeled, and adaptive sampling. Development of a suitable adaptive sampling algorithm was beyond the scope of this work; instead adaptive sampling was approximated by omitting the cost of samples discarded by the presented pivoted Cholesky approach.},
author = {Dalbey, Keith R},
file = {:home/gerardo/Dropbox/Sweden/papers/Efficient and Robust Gradient Enhanced Kriging Emulators.pdf:pdf},
number = {August},
title = {{Efficient and Robust Gradient Enhanced Kriging Emulators}},
url = {https://www.google.com/url?sa=t{\&}rct=j{\&}q={\&}esrc=s{\&}source=web{\&}cd=4{\&}cad=rja{\&}uact=8{\&}ved=0ahUKEwjT3ribqsLSAhWpzIMKHWQjB-IQFggtMAM{\&}url=http{\%}3A{\%}2F{\%}2Fprod.sandia.gov{\%}2Ftechlib{\%}2Faccess-control.cgi{\%}2F2013{\%}2F137022.pdf{\&}usg=AFQjCNGfWMoRRM6VCtBbUkG-1kz7NmFFUg{\&}sig2=cZ-},
year = {2013}
}
@article{Zheng2017,
abstract = {An efficient geometry optimization algorithm based on interpolated potential energy surfaces with iteratively updated Hessians is presented in this work. At each step of geometry optimization (including both minimization and transition structure search), an interpolated potential energy surface is properly constructed by using the previously calculated information (energies, gradients, and Hessians/updated Hessians), and Hessians of the two latest geometries are updated in an iterative manner. The optimized minimum or transition structure on the interpolated surface is used for the starting geometry of the next geometry optimization step. The cost of searching the minimum or transition structure on the interpolated surface and iteratively updating Hessians is usually negligible compared with most electronic structure single gradient calculations. These interpolated potential energy surfaces are often better representations of the true potential energy surface in a broader range than a local quadratic appr...},
author = {Zheng, Jingjing and Frisch, Michael J.},
doi = {10.1021/acs.jctc.7b00719},
file = {:home/gerardo/Dropbox/Sweden/papers/Efficient geometry minimization and transition structure optimization.pdf:pdf},
issn = {15499626},
journal = {Journal of Chemical Theory and Computation},
number = {12},
pages = {6424--6432},
title = {{Efficient Geometry Minimization and Transition Structure Optimization Using Interpolated Potential Energy Surfaces and Iteratively Updated Hessians}},
volume = {13},
year = {2017}
}
@article{Denzel2018a,
abstract = {We implemented a geometry optimizer based on Gaussian process regression (GPR) to find minimum structures on potential energy surfaces. We tested both a two times differentiable form of the Mat{\'{e}}rn kernel and the squared exponential kernel. The Mat{\'{e}}rn kernel performs much better. We give a detailed description of the optimization procedures. These include overshooting the step resulting from GPR in order to obtain a higher degree of interpolation vs. extrapolation. In a benchmark against the Limited-memory Broyden–Fletcher–Goldfarb–Shanno optimizer of the DL-FIND library on 26 test systems, we found the new optimizer to generally reduce the number of required optimization steps.},
author = {Denzel, Alexander and K{\"{a}}stner, Johannes},
doi = {10.1063/1.5017103},
file = {:home/gerardo/Dropbox/Sweden/papers/Gaussian process regression for geometry optimization.pdf:pdf},
issn = {00219606},
journal = {Journal of Chemical Physics},
number = {9},
title = {{Gaussian process regression for geometry optimization}},
volume = {148},
year = {2018}
}
@article{Denzel2018,
abstract = {We implemented a gradient-based algorithm for transition state search which uses Gaussian process regression. Besides a description of the algorithm, we provide a method to find the starting point for the optimization if only the reactant and product minima are known. We perform benchmarks on 27 test systems against the dimer method and partitioned rational function optimization as implemented in the DL-FIND library. We found the new optimizer to significantly decrease the number of required energy and gradient evaluations.},
author = {Denzel, Alexander and K{\"{a}}stner, Johannes},
doi = {10.1021/acs.jctc.8b00708},
file = {:home/gerardo/Dropbox/Sweden/papers/Gaussian Process Regression for Transition State Search.pdf:pdf},
issn = {15499626},
journal = {Journal of Chemical Theory and Computation},
number = {11},
pages = {5777--5786},
title = {{Gaussian Process Regression for Transition State Search}},
volume = {14},
year = {2018}
}
@book{Seeger2004,
abstract = {Gaussian processes (GPs) are natural generalisations of multivariate Gaussian random variables to infinite (countably or continuous) index sets. GPs have been applied in a large number of fields to a diverse range of ends, and very many deep theoretical analyses of various properties are available. This paper gives an introduction to Gaussian processes on a fairly elementary level with special emphasis on characteristics relevant in machine learning. It draws explicit connections to branches such as spline smoothing models and support vector machines in which similar ideas have been investigated. Gaussian process models are routinely used to solve hard machine learning problems. They are attractive because of their flexible non-parametric nature and computational simplicity. Treated within a Bayesian framework, very powerful statistical methods can be implemented which offer valid estimates of uncertainties in our predictions and generic model selection procedures cast as nonlinear optimization problems. Their main drawback of heavy computational scaling has recently been alleviated by the introduction of generic sparse approximations.13,78,31 The mathematical literature on GPs is large and often uses deep concepts which are not required to fully understand most machine learning applications. In this tutorial paper, we aim to present characteristics of GPs relevant to machine learning and to show up precise connections to other "kernel machines" popular in the community. Our focus is on a simple presentation, but references to more detailed sources are provided.},
author = {Seeger, Matthias},
booktitle = {International journal of neural systems},
doi = {10.1142/S0129065704001899},
file = {:home/gerardo/Dropbox/Sweden/papers/Gaussian Processes for Machine Learning:},
isbn = {026218253X},
issn = {0129-0657},
number = {2},
pages = {69--106},
pmid = {15112367},
title = {{Gaussian processes for machine learning.}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/15112367},
volume = {14},
year = {2004}
}
@article{Zielinski2017,
abstract = {{\textcopyright} 2017 The Author(s). The geometry optimization of a water molecule with a novel type of energy function called FFLUX is presented, which bypasses the traditional bonded potentials. Instead, topologically-partitioned atomic energies are trained by the machine learning method kriging to predict their IQA atomic energies for a previously unseen molecular geometry. Proof-of-concept that FFLUX's architecture is suitable for geometry optimization is rigorously demonstrated. It is found that accurate kriging models can optimize 2000 distorted geometries to within 0.28 kJ mol -1 of the corresponding ab initio energy, and 50{\%} of those to within 0.05 kJ mol -1 . Kriging models are robust enough to optimize the molecular geometry to sub-noise accuracy, when two thirds of the geometric inputs are outside the training range of that model. Finally, the individual components of the potential energy are analyzed, and chemical intuition is reflected in the independent behavior of the three energy terms E A E intra A (intra-atomic), V AA V cl AA ' (electrostatic) and V AA V x AA ' (exchange), in contrast to standard force fields.},
author = {Zielinski, Fran{\c{c}}ois and Maxwell, Peter I. and Fletcher, Timothy L. and Davie, Stuart J. and {Di Pasquale}, Nicodemo and Cardamone, Salvatore and Mills, Matthew J.L. and Popelier, Paul L.A.},
doi = {10.1038/s41598-017-12600-3},
file = {:home/gerardo/Dropbox/Sweden/papers/Geometry Optimization with Machine Trained Topological Atoms.pdf:pdf},
issn = {20452322},
journal = {Scientific Reports},
number = {1},
pages = {1--18},
title = {{Geometry Optimization with Machine Trained Topological Atoms}},
volume = {7},
year = {2017}
}
@article{DeBaar2013,
abstract = {{\textcopyright} 2014 by Begell House, Inc. Cokriging is a flexible tool for constructing surrogate models on the outputs of computer models. It can readily incorporate gradient information, in which form it is named gradient-enhanced Kriging (GEK), and promises accurate surrogate models in  {\textgreater} 10 dimensions with a moderate number of sample locations for sufficiently smooth responses. However, GEK suffers from several problems: poor robustness and ill-conditionedness of the surface. Furthermore it is unclear how to account for errors in gradients, which are typically larger than errors in values. In this work we derive GEK using Bayes' Theorem, which gives an useful interpretation of the method, allowing construction of a gradienterror contribution. The Bayesian interpretation suggests the “observation error” as a proxy for errors in the output of the computer model. From this point we derive analytic estimates of robustness of the method, which can easily be used to compute upper bounds on the correlation range and lower bounds on the observation error. We thus see that by including the observation error, treatment of errors and robustness go hand in hand. The resulting GEK method is applied to uncertainty quantification for two test problems.},
author = {de Baar, Jouke H.S. and Dwight, Richard P. and Bijl, Hester},
doi = {10.1615/int.j.uncertaintyquantification.2013006809},
file = {:home/gerardo/Dropbox/Sweden/papers/Improvements to gradient-enhanced kriging using a Bayesian Interpretation.pdf:pdf},
issn = {2152-5080},
journal = {International Journal for Uncertainty Quantification},
keywords = {fluid mechanics,gaussian random fields,maximum likelihood},
number = {3},
pages = {205--223},
title = {{Improvements To Gradient-Enhanced Kriging Using a Bayesian Interpretation}},
volume = {4},
year = {2013}
}
@article{Ulaganathan2016,
abstract = {{\textcopyright} 2015, Springer-Verlag London. The use of surrogate models for approximating computationally expensive simulations has been on the rise for the last two decades. Kriging-based surrogate models are popular for approximating deterministic computer models. In this work, the performance of Kriging is investigated when gradient information is introduced for the approximation of computationally expensive black-box simulations. This approach, known as gradient-enhanced Kriging, is applied to various benchmark functions of varying dimensionality (2D-20D). As expected, results from the benchmark problems show that additional gradient information can significantly enhance the accuracy of Kriging. Gradient-enhanced Kriging provides a better approximation even when gradient information is only partially available. Further comparison between gradient-enhanced Kriging and an alternative formulation of gradient-enhanced Kriging, called indirect gradient-enhanced Kriging, highlights various advantages of directly employing gradient information, such as improved surrogate model accuracy, better conditioning of the correlation matrix, etc. Finally, gradient-enhanced Kriging is used to model 6- and 10-variable fluid–structure interaction problems from bio-mechanics to identify the arterial wall's stiffness.},
author = {Ulaganathan, Selvakumar and Couckuyt, Ivo and Dhaene, Tom and Degroote, Joris and Laermans, Eric},
doi = {10.1007/s00366-015-0397-y},
file = {:home/gerardo/Dropbox/Sweden/papers/Performance study of Gradient Enhanced Kriging.pdf:pdf},
issn = {14355663},
journal = {Engineering with Computers},
keywords = {Fluid structure interaction,Gradient enhancement,Kriging,Surrogate modelling},
number = {1},
pages = {15--34},
publisher = {Springer London},
title = {{Performance study of gradient-enhanced Kriging}},
url = {http://dx.doi.org/10.1007/s00366-015-0397-y},
volume = {32},
year = {2016}
}
@article{Lindh1999,
abstract = {A novel procedure to select internal coordinates for molecular geometry optimizations is presented. The procedure has features in common with other so-called redundant internal coordinates schemes. It is a black-box method which automatically selects an appropriate set of internal coordinates in which the geometry optimization is performed. The method is explicitly expressed in the non-redundant parameter space, thus avoiding the need for projections from the redundant internal coordinate space. The new procedure introduces a weighting in which the redundancy is modified prior to the generation of the non-redundant internal coordinates. The new method favors those redundant internal coordinates which are the most significant. It has favorable properties for the automatic generation of molecular coordinates in van der Waals complexes and transition state optimizations.},
author = {Lindh, Roland and Bernhardsson, Anders and Sch{\"{u}}tz, Martin},
doi = {10.1016/S0009-2614(99)00247-X},
file = {:home/gerardo/Dropbox/Sweden/papers/Force-constant weighted redundant coordinates in molecular geometry optimizations.pdf:pdf},
issn = {00092614},
journal = {Chemical Physics Letters},
number = {5-6},
pages = {567--575},
title = {{Force-constant weighted redundant coordinates in molecular geometry optimizations}},
volume = {303},
year = {1999}
}
@book{Stein2019,
abstract = {Prediction of a random field based on observations of the random field at some set of locations arises in mining, hydrology, atmospheric sciences, and geography. Kriging, a prediction scheme defined as any prediction scheme that minimizes mean squared prediction error among some class of predictors under a particular model for the field, is commonly used in all these areas of prediction. This book summarizes past work and describes new approaches to thinking about kriging.},
address = {NY},
author = {Stein, Michael L.},
booktitle = {Interpolation of Spatial Data},
doi = {10.1007/978-1-4612-1494-6},
file = {:home/gerardo/Dropbox/Sweden/papers/interpolation of spatial data.pdf:pdf},
isbn = {978-1-4612-7166-6},
issn = {25741241},
month = {sep},
publisher = {Springer Science+Business Media New York},
series = {Springer Series in Statistics},
title = {{Interpolation of Spatial Data}},
url = {https://biomedres.us/fulltexts/BJSTR.MS.ID.001746.php http://link.springer.com/10.1007/978-1-4612-1494-6},
volume = {9},
year = {1999}
}
@inproceedings{Yamazaki2012,
address = {Reston, Virigina},
author = {Yamazaki, Wataru and Rumpfkeil, Markus and Mavriplis, Dimitri},
booktitle = {28th AIAA Applied Aerodynamics Conference},
doi = {10.2514/6.2010-4363},
file = {:home/gerardo/Dropbox/Sweden/papers/Design Optimization Utilizing Gradient-Hessian Enhanced Surrogate Model .pdf:pdf},
isbn = {978-1-62410-141-0},
month = {jun},
number = {July},
pages = {1--23},
publisher = {American Institute of Aeronautics and Astronautics},
title = {{Design Optimization Utilizing Gradient/Hessian Enhanced Surrogate Model}},
url = {http://arc.aiaa.org/doi/10.2514/6.2010-4363},
year = {2010}
}
@book{lapack1999,
author = {Anderson, E. and Bai, Z. and Bischof, C. Blackford, S. and Demmel, J. and Dongarra, J. and Du Croz, J. and Greenbaum, A. and Hammarling, S. and McKenney, A. and Sorensen, D.},
TITLE = {{LAPACK} Users' Guide},
EDITION = {Third},
PUBLISHER = {Society for Industrial and Applied Mathematics},
YEAR = {1999},
ADDRESS = {Philadelphia, PA},
ISBN = {0-89871-447-8 (paperback)},
url = {http://www.netlib.org/lapack/faq.html}
}
